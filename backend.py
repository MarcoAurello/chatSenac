from pathlib import Path
import random
import os

import streamlit as st
from dotenv import load_dotenv, find_dotenv

from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain

# üîê Carrega vari√°veis de ambiente
_ = load_dotenv(find_dotenv())

# üìÅ Diret√≥rio onde os PDFs ser√£o armazenados
folder_files = Path(__file__).parent / "files"
model_name = "gpt-3.5-turbo-0125"
DEBUG = False  # Ativa logs no console do Streamlit para debug

# üìÑ Fun√ß√£o para importar documentos
def importar_documentos() -> list:
    documentos = []
    for arquivo in folder_files.glob("*.pdf"):
        if DEBUG: st.write(f"üîç Carregando {arquivo.name}")
        loader = PyPDFLoader(arquivo)
        documentos.extend(loader.load())
    return documentos

# ‚úÇÔ∏è Fun√ß√£o para dividir documentos
def dividir_documentos(documentos: list) -> list:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    documentos_divididos = splitter.split_documents(documentos)

    for i, doc in enumerate(documentos_divididos):
        doc.metadata["source"] = Path(doc.metadata.get("source", "Desconhecido")).name
        doc.metadata["doc_id"] = i

    return documentos_divididos

# üß† Cria o vector store (FAISS)
def criar_vector_store(documentos):
    embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
    vector_store = FAISS.from_documents(
        documents=documentos,
        embedding=embedding_model
    )
    return vector_store

# üéØ Gera perguntas de quiz (n√£o alterado)
def gerar_perguntas_quiz(documentos, qtd_perguntas=10):
    chat = ChatOpenAI(model=model_name)
    random.shuffle(documentos)
    trechos_selecionados = documentos[:min(qtd_perguntas, len(documentos))]
    texto_base = "\n".join([doc.page_content for doc in trechos_selecionados])

    prompt = f"""
    A partir do texto abaixo, gere {qtd_perguntas} perguntas de m√∫ltipla escolha com 4 alternativas cada.

    Para cada pergunta, siga este formato:
    Pergunta: [texto da pergunta]
    A) [op√ß√£o 1]
    B) [op√ß√£o 2]
    C) [op√ß√£o 3]
    D) [op√ß√£o 4]
    Resposta: [letra da op√ß√£o correta]
    Explica√ß√£o: [breve explica√ß√£o de por que essa √© a resposta correta]

    Texto base:
    {texto_base}
    """

    resposta = chat.invoke(prompt)
    return resposta.content

# üî• Fun√ß√£o para gerar o prompt dinamicamente
def gerar_prompt_dinamico():
    # 30% de chance de fazer uma pergunta reflexiva
    fazer_reflexao = random.random() < 0.3

    # For√ßar reflex√£o ap√≥s 3 intera√ß√µes sem reflex√£o
    if st.session_state.interacoes_sem_reflexao >= 3:
        fazer_reflexao = True

    base_prompt = """
Voc√™ √© um tutor paciente conversando com um aluno.

Use o seguinte conte√∫do dos documentos para responder:
{context}

Baseado nisso, e no hist√≥rico de conversa:
{chat_history}

E na nova pergunta:
{question}

Responda de maneira clara, did√°tica e amig√°vel.
"""

    if fazer_reflexao:
        base_prompt += """
Depois de responder, estimule o aluno a refletir com uma pergunta aberta como:
- "O que voc√™ acha sobre isso?"
- "Por que voc√™ acredita que isso acontece?"
- "Voc√™ conseguiria pensar em um exemplo onde isso se aplica?"
"""
        # Resetar contador
        st.session_state.interacoes_sem_reflexao = 0
    else:
        # Aumenta contador
        st.session_state.interacoes_sem_reflexao += 1

    base_prompt += """
Agora responda:
"""

    return PromptTemplate(
        input_variables=["chat_history", "question", "context"],
        template=base_prompt
    )

# üöÄ Fun√ß√£o principal para criar o chain
def cria_chain_conversa():
    documentos = importar_documentos()
    documentos = dividir_documentos(documentos)
    vector_store = criar_vector_store(documentos)

    chat_model = ChatOpenAI(model=model_name)

    memory = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

    retriever = vector_store.as_retriever()

    # Inicializa contadores de intera√ß√£o
    if "num_interacoes" not in st.session_state:
        st.session_state.num_interacoes = 0
    if "interacoes_sem_reflexao" not in st.session_state:
        st.session_state.interacoes_sem_reflexao = 0

    # Cria o chain (sem prompt fixo ainda)
    chain = ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        memory=memory,
        retriever=retriever,
        return_source_documents=True,
        verbose=DEBUG
    )

    st.session_state["chain"] = chain

    return chain

# üó®Ô∏è Fun√ß√£o para interagir com o usu√°rio (com atualiza√ß√£o din√¢mica do prompt)
def responder_usuario(pergunta_usuario):
    # Gera novo prompt din√¢mico a cada intera√ß√£o
    prompt_template = gerar_prompt_dinamico()

    # Atualiza o prompt do chain em runtime
    st.session_state["chain"].combine_docs_chain.llm_chain.prompt = prompt_template

    resposta = st.session_state["chain"].invoke({"question": pergunta_usuario})

    # Atualiza contador total de intera√ß√µes
    st.session_state.num_interacoes += 1

    return resposta["answer"]
