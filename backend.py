from pathlib import Path
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain_openai.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
import random
import streamlit as st
from dotenv import load_dotenv, find_dotenv

# üîê Carrega vari√°veis de ambiente
_ = load_dotenv(find_dotenv())

# üìÅ Diret√≥rio onde os PDFs ser√£o armazenados
folder_files = Path(__file__).parent / "files"
model_name = "gpt-3.5-turbo-0125"
DEBUG = False  # Ativa logs no console do Streamlit para debug

# üìÑ Carrega todos os documentos PDF da pasta
def importar_documentos() -> list:
    documentos = []
    for arquivo in folder_files.glob("*.pdf"):
        if DEBUG: st.write(f"üîç Carregando {arquivo.name}")
        loader = PyPDFLoader(arquivo)
        documentos_arquivo = loader.load()
        documentos.extend(documentos_arquivo)
    return documentos

# ‚úÇÔ∏è Divide documentos em peda√ßos menores com sobreposi√ß√£o
def dividir_documentos(documentos: list) -> list:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    documentos_divididos = splitter.split_documents(documentos)

    for i, doc in enumerate(documentos_divididos):
        doc.metadata["source"] = Path(doc.metadata.get("source", "Desconhecido")).name
        doc.metadata["doc_id"] = i

    return documentos_divididos

# üß† Cria um banco vetorial FAISS com embeddings OpenAI
def criar_vector_store(documentos):
    embedding_model = OpenAIEmbeddings()
    vector_store = FAISS.from_documents(
        documents=documentos,
        embedding=embedding_model
    )
    return vector_store

# üéØ Gera perguntas de quiz com varia√ß√£o aleat√≥ria
def gerar_perguntas_quiz(documentos, qtd_perguntas=5):
    chat = ChatOpenAI(model=model_name)

    random.shuffle(documentos)
    trechos_selecionados = documentos[:min(5, len(documentos))]
    texto_base = "\n".join([doc.page_content for doc in trechos_selecionados])

    prompt = f"""
    A partir do texto abaixo, gere {qtd_perguntas} perguntas de m√∫ltipla escolha com 4 alternativas cada.

    Para cada pergunta, siga este formato:
    Pergunta: [texto da pergunta]
    A) [op√ß√£o 1]
    B) [op√ß√£o 2]
    C) [op√ß√£o 3]
    D) [op√ß√£o 4]
    Resposta: [letra da op√ß√£o correta]
    Explica√ß√£o: [breve explica√ß√£o de por que essa √© a resposta correta]

    Texto base:
    {texto_base}
    """

    resposta = chat.invoke(prompt)
    return resposta.content

# ü§ñ Cria a cadeia de conversa com mem√≥ria e busca sem√¢ntica
def cria_chain_conversa():
    documentos = importar_documentos()
    documentos = dividir_documentos(documentos)
    vector_store = criar_vector_store(documentos)

    chat_model = ChatOpenAI(model=model_name)

    memory = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

    retriever = vector_store.as_retriever()

    chain = ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        memory=memory,
        retriever=retriever,
        return_source_documents=True,
        verbose=DEBUG
    )

    st.session_state["chain"] = chain
    return chain
