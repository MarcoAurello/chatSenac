from pathlib import Path
import random
import os

import streamlit as st
from dotenv import load_dotenv, find_dotenv

from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain

# üîê Carrega vari√°veis de ambiente
_ = load_dotenv(find_dotenv())

# üìÅ Diret√≥rio onde os PDFs ser√£o armazenados
folder_files = Path(__file__).parent / "files"
model_name = "gpt-3.5-turbo-0125"
DEBUG = False  # Ativa logs no console do Streamlit para debug

# üìÑ Fun√ß√£o para importar documentos
def importar_documentos() -> list:
    documentos = []
    session_id = st.session_state.get("session_id", "")

    for arquivo in folder_files.glob(f"*_{session_id}.pdf"):
        loader = PyPDFLoader(str(arquivo))
        documentos.extend(loader.load())

    return documentos

# ‚úÇÔ∏è Fun√ß√£o para dividir documentos
def dividir_documentos(documentos: list) -> list:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    documentos_divididos = splitter.split_documents(documentos)

    for i, doc in enumerate(documentos_divididos):
        doc.metadata["source"] = Path(doc.metadata.get("source", "Desconhecido")).name
        doc.metadata["doc_id"] = i

    return documentos_divididos

# üß† Cria o vector store (FAISS)
def criar_vector_store(documentos):
    if not documentos:
        st.error("‚ùå Nenhum documento foi fornecido para criar o vetor store.")
        return None

    # Verifica√ß√£o opcional: garantir que todos os documentos t√™m conte√∫do textual
    for i, doc in enumerate(documentos):
        if not hasattr(doc, 'page_content') and not hasattr(doc, 'text'):
            st.error(f"‚ùå Documento na posi√ß√£o {i} n√£o possui texto v√°lido.")
            return None

    embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

    # Cria√ß√£o segura do vector store
    try:
        vector_store = FAISS.from_documents(
            documents=documentos,
            embedding=embedding_model
        )
    except IndexError as e:
        st.error("‚ùå Erro ao criar o √≠ndice FAISS. Verifique se os documentos t√™m conte√∫do v√°lido.")
        return None

    return vector_store


# üéØ Gera perguntas de quiz (n√£o alterado)
def gerar_perguntas_quiz(documentos, qtd_perguntas=10):
    chat = ChatOpenAI(model=model_name)
    random.shuffle(documentos)
    trechos_selecionados = documentos[:min(qtd_perguntas, len(documentos))]
    texto_base = "\n".join([doc.page_content for doc in trechos_selecionados])

    prompt = f"""
    A partir do texto abaixo, gere {qtd_perguntas} perguntas de m√∫ltipla escolha com 4 alternativas cada.

    Para cada pergunta, siga este formato:
    Pergunta: [texto da pergunta]
    A) [op√ß√£o 1]
    B) [op√ß√£o 2]
    C) [op√ß√£o 3]
    D) [op√ß√£o 4]
    Resposta: [letra da op√ß√£o correta]
    Explica√ß√£o: [breve explica√ß√£o de por que essa √© a resposta correta]

    Texto base:
    {texto_base}
    """

    resposta = chat.invoke(prompt)
    return resposta.content

# üî• Fun√ß√£o para gerar o prompt dinamicamente
def gerar_prompt_dinamico():
    # 30% de chance de fazer uma pergunta reflexiva
    fazer_reflexao = random.random() < 0.3

    # For√ßar reflex√£o ap√≥s 3 intera√ß√µes sem reflex√£o
    if st.session_state.interacoes_sem_reflexao >= 3:
        fazer_reflexao = True

    base_prompt = """
Voc√™ √© um professor dedicado que busca n√£o apenas transmitir conhecimento, mas tamb√©m provocar reflex√µes e incentivar a constru√ß√£o do saber junto ao aluno.
Adapte sua linguagem conforme o n√≠vel de entendimento demonstrado pelo aluno. Use exemplos simples, analogias do cotidiano e perguntas para manter o aluno engajado e garantir compreens√£o.

Use o seguinte conte√∫do dos documentos para responder:
{context}

Baseado nisso, e no hist√≥rico da conversa:
{chat_history}

E na nova pergunta:
{question}

Se o aluno demonstrar d√∫vidas ou dificuldades, revisite os conceitos b√°sicos de forma acess√≠vel.
Responda de forma did√°tica, amig√°vel e envolvente. Seu objetivo √© n√£o s√≥ responder, mas ajudar o aluno a aprender de verdade.
"""

    if fazer_reflexao:
        base_prompt += """
Ap√≥s sua explica√ß√£o, estimule a reflex√£o do aluno com uma pergunta aberta como:
- "O que voc√™ acha sobre isso?"
- "Por que voc√™ acredita que isso acontece?"
- "Voc√™ conseguiria pensar em um exemplo onde isso se aplica?"
- "Voc√™ j√° viveu algo parecido com isso?"
- "Consegue pensar em como isso se relaciona com o seu dia a dia?"
- "Qual parte te chamou mais aten√ß√£o? Por qu√™?"
"""
        st.session_state.interacoes_sem_reflexao = 0
    else:
        st.session_state.interacoes_sem_reflexao += 1

    base_prompt += """
Agora responda:
"""

    return PromptTemplate(
        input_variables=["chat_history", "question", "context"],
        template=base_prompt
    )

# üöÄ Fun√ß√£o principal para criar o chain
def cria_chain_conversa():
    documentos = importar_documentos()

    if not documentos:
        st.session_state.erro_chat = "‚ùå Nenhum arquivo encontrado para inicializar o chat. Por favor, carregue um arquivo PDF."
        return None

    documentos = dividir_documentos(documentos)
    vector_store = criar_vector_store(documentos)

    if vector_store is None:
        st.session_state.erro_chat = "‚ùå N√£o foi poss√≠vel criar o vector store. Verifique os documentos."
        return None

    chat_model = ChatOpenAI(model=model_name)

    memory = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

    try:
        retriever = vector_store.as_retriever()
    except AttributeError:
        st.session_state.erro_chat = "‚ùå O vector store n√£o possui o m√©todo 'as_retriever'. Verifique a cria√ß√£o do vector store."
        return None

    # Limpa mensagens de erro anteriores, se houve sucesso at√© aqui
    st.session_state.pop("erro_chat", None)

    # Inicializa contadores de intera√ß√£o
    if "num_interacoes" not in st.session_state:
        st.session_state.num_interacoes = 0
    if "interacoes_sem_reflexao" not in st.session_state:
        st.session_state.interacoes_sem_reflexao = 0

    # Gera o prompt din√¢mico inicial
    prompt_template = gerar_prompt_dinamico()

    # Cria o chain com prompt personalizado
    chain = ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        memory=memory,
        retriever=retriever,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt_template},
        verbose=DEBUG
    )

    st.session_state["chain"] = chain

    return chain

# üó®Ô∏è Fun√ß√£o para interagir com o usu√°rio (com atualiza√ß√£o din√¢mica do prompt)
def responder_usuario(pergunta_usuario):
    # Gera novo prompt din√¢mico a cada intera√ß√£o
    prompt_template = gerar_prompt_dinamico()

    # Atualiza o prompt do chain em runtime
    st.session_state["chain"].combine_docs_chain.llm_chain.prompt = prompt_template

    resposta = st.session_state["chain"].invoke({"question": pergunta_usuario})

    # Atualiza contador total de intera√ß√µes
    st.session_state.num_interacoes += 1

    return resposta["answer"]
